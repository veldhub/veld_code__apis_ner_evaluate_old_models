{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of spacy 2.2.4 models by Stefan\n",
    "\n",
    "There are 7 trained models. 6 of which have evaluation data in their folder.\n",
    "\n",
    "This evaluation data exists in these formats:\n",
    "- txt (1x: 2019-12-03)\n",
    "- pickle (4x: 2020-01-02 - 2020-04-16)\n",
    "- json (1x: 2020-04-30)\n",
    "\n",
    "For the `txt` and `pickle` files there are parsing functions scattered around this repo. Importing\n",
    "them from their modules however causes execution of the modules which leads to crashes as a lot of\n",
    "context is missing. In order to avoid changes to an undocumented and sprawled codebase, but still\n",
    "to reuse code and its context, relevant code was copied / imported here. For the `json` file, no\n",
    "existing parsing function was found, so one was implemented.\n",
    "\n",
    "There were also existing evaluation functions compatible with the pickle files of the models\n",
    "2020-01-02 - 2020-04-16, so that was copied here and reused. For the others, custom evaluation\n",
    "logic was implemented, using spaCy's function on the `txt` and a custom one on the `json` file as\n",
    "that data was a in shape incompatible with spaCy's function.\n",
    "\n",
    "The commit of the remaining code base which was copied or imported from is\n",
    "`8e75d3561e617f1bd135d4c06fbb982285f6f544`\n",
    "\n",
    "## Evaluation summary\n",
    "\n",
    "- 2019-12-03\n",
    "  - spaCy's evaluation: **p: 53.84, r: 51.28**\n",
    "  - evaluation file: `ner_apis_2019-12-03_23:32:24/corpus/evalset.txt`\n",
    "  - evaluation data size: 3042 sentences, 1657 assigned tags\n",
    "  - NER tags: 'LOC', 'MISC', 'ORG', 'PER'\n",
    "- 2020-01-02\n",
    "  - EM (manual evaluation): **p: 76.86, r: 38.92**\n",
    "  - E0 (spaCy's evaluation, with 'ner' pipeline only): **p: 61.78, r: 47.92**\n",
    "  - E1 (spaCy's evaluation, with all pipelines): throws errors\n",
    "  - evaluation file: `ner_apis_2020-01-02_12:34:48/corpus/evalset.pickle`\n",
    "  - evaluation data size: 2474 sentences, 1647 assigned tags\n",
    "  - NER tags: 'LOC', 'ORG'\n",
    "- 2020-01-29\n",
    "  - EM (manual evaluation):  **p: 72.04, r: 74.83**\n",
    "  - E0 (spaCy's evaluation, with 'ner' pipeline only): **p: 76.41, r: 71.47**\n",
    "  - E1 (spaCy's evaluation, with all pipelines): throws errors\n",
    "  - evaluation file: `ner_apis_2020-01-29_13:19:53/corpus/evalset.pickle`\n",
    "  - evaluation data size: 832 sentences, 1557 assigned tags\n",
    "  - NER tags: 'LOC', 'MISC', 'ORG', 'PER'\n",
    "- 2020-04-07\n",
    "  - EM (manual evaluation): **p: 72.44, r: 82.33**\n",
    "  - E0 (spaCy's evaluation, with 'ner' pipeline only): **p: 77.70, r: 81.18**\n",
    "  - E1 (spaCy's evaluation, with all pipelines): **p: 75.62, r: 77.63**\n",
    "  - evaluation file: `ner_apis_2020-04-07_15:00:35/corpus/evalset.pickle`\n",
    "  - evaluation data size: 861 sentences, 1804 assigned tags\n",
    "  - NER tags: 'LOC', 'MISC', 'ORG', 'PER'\n",
    "- 2020-04-16\n",
    "  - EM (manual evaluation): **p: 53.37, r: 49.37**\n",
    "  - E0 (spaCy's evaluation, with 'ner' pipeline only): **p: 54.81, r: 43.24**\n",
    "  - E1 (spaCy's evaluation, with all pipelines): **p: 54.18, r: 41.84**\n",
    "  - evaluation file: `ner_apis_2020-04-16_14:21:46/corpus/evalset.pickle`\n",
    "  - evaluation data size: 866 sentences, 1878 assigned tags\n",
    "  - NER tags: 'LOC', 'MISC', 'ORG', 'PER'\n",
    "- 2020-04-30\n",
    "  - manual evaluation: **p: 81.76, r: 23.59**\n",
    "  - evaluation file: ` ner_apis_2020-04-30_11:24:09/corpus/evalset.json`\n",
    "  - evaluation data size: 904 sentences, 3144 assigned tags (BILOU tags, so plenty of redundancies)\n",
    "  - NER tags: 'LOC', 'MISC', 'ORG', 'PER'\n",
    "\n",
    "## How to reproduce\n",
    "\n",
    "I don't know the exact versions of the python interpreter and packages that were used to build this\n",
    "codebase, but I could get the model loading and this evaluation here running by using:\n",
    "\n",
    "- python 3.6.9 (needed for the unpickling of some eval data sets, 3.8 crashed)\n",
    "- poetry 1.3.2\n",
    "\n",
    "and the `pyproject.toml` and `poetry.lock` in this folder. To run this, install poetry, go into this\n",
    "folder and do: `poetry install`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "# __sresch__ these imports need to be done, because otherwise the unpickling of metadata would crash\n",
    "# in the function NERer.load_metadata . No idea why.\n",
    "from ner.model_ner import ModelType, TrainingStyle\n",
    "import ner.model_ner\n",
    "\n",
    "\n",
    "# this is necessary to avoid this strange E050\n",
    "# related: https://github.com/explosion/spaCy/issues/3552\n",
    "nlp = spacy.load('de_core_news_md')\n",
    "del nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER tags: ('LOC', 'MISC', 'ORG', 'PER')\n",
      "number of tags in evaluation data: 1657\n",
      "number of sentences in evaluation data: 3042\n",
      "p: 53.84615384615385, r: 51.28712871287129\n"
     ]
    }
   ],
   "source": [
    "# evaluate model 2019_12_03\n",
    "\n",
    "\n",
    "# __sresch__ this function is copied without modifications from 'NER Place Institution.ipynb'\n",
    "# because since it's in a jupyter notebook, it couldn't be easily imported without third party\n",
    "# tooling. And if it the import was made possible, it executed the whole other notebook.\n",
    "def read_data_from_txt(mypath):\n",
    "    mydata = []\n",
    "    with open(mypath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            while lines[i].isspace():\n",
    "                i += 1\n",
    "            # we found a non-empty line to use for t\n",
    "            t = lines[i].strip()\n",
    "            i += 1\n",
    "            while lines[i].isspace():\n",
    "                i += 1\n",
    "            # we found a non-empty line to use for e if possible, else for t\n",
    "            e = None\n",
    "            while e == None:\n",
    "                try:\n",
    "                    e = eval(lines[i])\n",
    "                except SyntaxError:\n",
    "                    t += lines[i].strip()\n",
    "                    i += 1\n",
    "                    while lines[i].isspace():\n",
    "                        i += 1\n",
    "                    # we found a non-empty line to try to use for e again\n",
    "            i += 1\n",
    "            mydata.append( (t, e, None, None) )\n",
    "    return mydata\n",
    "\n",
    "# __sresch__ custom code, utilizing the given txt reader function above made for that data shape\n",
    "model_dir = \"/veld/input/ner_apis_2019-12-03_23:32:24\"\n",
    "nlp = spacy.load(f\"{model_dir}/nlp\")\n",
    "print(f\"NER tags: {nlp.get_pipe('ner').labels}\")\n",
    "eval_data = read_data_from_txt(f\"{model_dir}/corpus/evalset.txt\")\n",
    "print(f\"number of tags in evaluation data: {sum([len(e[1]['entities']) for e in eval_data])}\")\n",
    "print(f\"number of sentences in evaluation data: {len(eval_data)}\")\n",
    "eval_data_spacy = [(s,e) for s, e, _, _ in eval_data]\n",
    "scorer = nlp.evaluate(eval_data_spacy, verbose=False)\n",
    "print(f\"p: {scorer.ents_p}, r: {scorer.ents_r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model at ner_apis_2020-01-02_12:34:48 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steff-vm/veld/exp/veld_experimental_repos/veld_apis_ner_chain_1_eval_old_models/spacy-ner/notebooks__spacy2.2.4/ner/model_ner.py:72: ResourceWarning: unclosed file <_io.BufferedReader name='../ner_apis_2020-01-02_12:34:48/model_dict.pickle'>\n",
      "  dict = pickle.load(open(path, \"rb\"))\n",
      "/home/steff-vm/.cache/pypoetry/virtualenvs/spacy-ner-Js5Rfv7x-py3.6/lib/python3.6/site-packages/srsly/cloudpickle/cloudpickle.py:798: UserWarning: A pickle file created using an old (<=1.4.1) version of cloudpickle is currently being loaded. This is not supported by cloudpickle and will break in cloudpickle 1.7\n",
      "  \"will break in cloudpickle 1.7\", category=UserWarning\n",
      "/home/steff-vm/veld/exp/veld_experimental_repos/veld_apis_ner_chain_1_eval_old_models/spacy-ner/notebooks__spacy2.2.4/ner/model_ner.py:84: ResourceWarning: unclosed file <_io.BufferedReader name='../ner_apis_2020-01-02_12:34:48/corpus/evalset.pickle'>\n",
      "  data_without_goldparse = pickle.load(open(path, \"rb\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model.\n",
      "NER tags: ('LOC', 'ORG')\n",
      "number of tags in evaluation data: 1647\n",
      "number of sentences in evaluation data: 2474\n",
      "Starting EM (manual evaluation) ...\n",
      "EM: ents_p=76.86472819216182, ents_r=38.92445582586428\n",
      "Running spacy's evaluation (E0) with (string, GoldParse) as input over only the 'ner' pipe ...\n",
      "The abbreviations files during data extraction and model training did not match. I will now remove the datapoints from the evaluation set whose tokenization differs.\n",
      "I remove 1153 datapoints from the evaluation data.\n",
      "Trying the evaluation again ...\n",
      "E0: ents_p=61.78571428571429, ents_r=47.92243767313019\n",
      "Loading model at ner_apis_2020-01-29_13:19:53 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steff-vm/veld/exp/veld_experimental_repos/veld_apis_ner_chain_1_eval_old_models/spacy-ner/notebooks__spacy2.2.4/ner/model_ner.py:72: ResourceWarning: unclosed file <_io.BufferedReader name='../ner_apis_2020-01-29_13:19:53/model_dict.pickle'>\n",
      "  dict = pickle.load(open(path, \"rb\"))\n",
      "/home/steff-vm/veld/exp/veld_experimental_repos/veld_apis_ner_chain_1_eval_old_models/spacy-ner/notebooks__spacy2.2.4/ner/model_ner.py:84: ResourceWarning: unclosed file <_io.BufferedReader name='../ner_apis_2020-01-29_13:19:53/corpus/evalset.pickle'>\n",
      "  data_without_goldparse = pickle.load(open(path, \"rb\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model.\n",
      "NER tags: ('LOC', 'MISC', 'ORG', 'PER')\n",
      "number of tags in evaluation data: 1557\n",
      "number of sentences in evaluation data: 832\n",
      "Starting EM (manual evaluation) ...\n",
      "EM: ents_p=72.04152249134948, ents_r=74.83824586628324\n",
      "Running spacy's evaluation (E0) with (string, GoldParse) as input over only the 'ner' pipe ...\n",
      "The abbreviations files during data extraction and model training did not match. I will now remove the datapoints from the evaluation set whose tokenization differs.\n",
      "I remove 515 datapoints from the evaluation data.\n",
      "Trying the evaluation again ...\n",
      "E0: ents_p=76.41509433962264, ents_r=71.47058823529412\n",
      "Loading model at ner_apis_2020-04-07_15:00:35 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steff-vm/veld/exp/veld_experimental_repos/veld_apis_ner_chain_1_eval_old_models/spacy-ner/notebooks__spacy2.2.4/ner/model_ner.py:72: ResourceWarning: unclosed file <_io.BufferedReader name='../ner_apis_2020-04-07_15:00:35/model_dict.pickle'>\n",
      "  dict = pickle.load(open(path, \"rb\"))\n",
      "/home/steff-vm/veld/exp/veld_experimental_repos/veld_apis_ner_chain_1_eval_old_models/spacy-ner/notebooks__spacy2.2.4/ner/model_ner.py:84: ResourceWarning: unclosed file <_io.BufferedReader name='../ner_apis_2020-04-07_15:00:35/corpus/evalset.pickle'>\n",
      "  data_without_goldparse = pickle.load(open(path, \"rb\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model.\n",
      "NER tags: ('LOC', 'MISC', 'ORG', 'PER')\n",
      "number of tags in evaluation data: 1804\n",
      "number of sentences in evaluation data: 861\n",
      "Starting EM (manual evaluation) ...\n",
      "EM: ents_p=72.44565217391305, ents_r=82.33477455219271\n",
      "Running spacy's evaluation (E0) with (string, GoldParse) as input over only the 'ner' pipe ...\n",
      "The abbreviations files during data extraction and model training did not match. I will now remove the datapoints from the evaluation set whose tokenization differs.\n",
      "I remove 66 datapoints from the evaluation data.\n",
      "Trying the evaluation again ...\n",
      "E0: ents_p=77.705112960761, ents_r=81.18012422360248\n",
      "Running spacy's evaluation (E1) with (string, GoldParse) as input over the pipes 'tagger', 'parser', and 'ner' ...\n",
      "The abbreviations files during data extraction and model training did not match. I will now remove the datapoints from the evaluation set whose tokenization differs.\n",
      "I remove 66 datapoints from the evaluation data.\n",
      "Trying the evaluation again ...\n",
      "E1: ents_p=75.62008469449486, ents_r=77.63975155279503\n",
      "Loading model at ner_apis_2020-04-16_14:21:46 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steff-vm/veld/exp/veld_experimental_repos/veld_apis_ner_chain_1_eval_old_models/spacy-ner/notebooks__spacy2.2.4/ner/model_ner.py:72: ResourceWarning: unclosed file <_io.BufferedReader name='../ner_apis_2020-04-16_14:21:46/model_dict.pickle'>\n",
      "  dict = pickle.load(open(path, \"rb\"))\n",
      "/home/steff-vm/veld/exp/veld_experimental_repos/veld_apis_ner_chain_1_eval_old_models/spacy-ner/notebooks__spacy2.2.4/ner/model_ner.py:84: ResourceWarning: unclosed file <_io.BufferedReader name='../ner_apis_2020-04-16_14:21:46/corpus/evalset.pickle'>\n",
      "  data_without_goldparse = pickle.load(open(path, \"rb\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model.\n",
      "NER tags: ('LOC', 'MISC', 'ORG', 'PER')\n",
      "number of tags in evaluation data: 1878\n",
      "number of sentences in evaluation data: 866\n",
      "Starting EM (manual evaluation) ...\n",
      "EM: ents_p=53.37837837837838, ents_r=49.375\n",
      "Running spacy's evaluation (E0) with (string, GoldParse) as input over only the 'ner' pipe ...\n",
      "E0: ents_p=54.81283422459893, ents_r=43.24894514767932\n",
      "Running spacy's evaluation (E1) with (string, GoldParse) as input over the pipes 'tagger', 'parser', and 'ner' ...\n",
      "E1: ents_p=54.189435336976324, ents_r=41.842475386779185\n"
     ]
    }
   ],
   "source": [
    "# evaluate models 2020_01_02, 2020-01-29, 2020-04-07, 2020-04-16\n",
    "\n",
    "\n",
    "# __sresch__ copied into here from evaluate_model.py as it could not be imported without causing\n",
    "# execution of the imported modules.\n",
    "def evaluate_without_tokenization_mismatches(nerer, pipes_to_disable):\n",
    "    print(\"The abbreviations files during data extraction and model training did not match. \" +\n",
    "        \"I will now remove the datapoints from the evaluation set whose tokenization differs.\")\n",
    "    with nerer.nlp.disable_pipes(*pipes_to_disable):\n",
    "        sent_doc_gp = [ (d.sentence, nerer.nlp(d.sentence), d.goldparse) for d in nerer.evaluation_data ]\n",
    "    num_prob = 0\n",
    "    ok = []\n",
    "    for s,d,g in sent_doc_gp:\n",
    "        if len(d) != len(g):\n",
    "            num_prob += 1\n",
    "        else:\n",
    "            ok.append( (s,g) )\n",
    "    print(f\"I remove {num_prob} datapoints from the evaluation data.\")\n",
    "    print(f\"Trying the evaluation again ...\")\n",
    "    # copy here what nerer.evaluate() does:\n",
    "    with nerer.nlp.disable_pipes(*pipes_to_disable):\n",
    "        scorer = nerer.nlp.evaluate(ok, verbose=False)\n",
    "        nerer.scorer = scorer\n",
    "        nerer.scores = scorer.scores\n",
    "\n",
    "# __sresch__ custom data structure to avoid the hardwired on in evaluate_models.py\n",
    "evaluations = [\n",
    "    {\n",
    "        \"model_dir\": \"ner_apis_2020-01-02_12:34:48\",\n",
    "        \"run_eval_manual\": True,\n",
    "        \"run_eval_0\": True,\n",
    "         # run_eval_1 would crash because tagger pipeline is missing in model, it's disabled\n",
    "        \"run_eval_1\": False,\n",
    "    },\n",
    "    {\n",
    "        \"model_dir\": \"ner_apis_2020-01-29_13:19:53\",\n",
    "        \"run_eval_manual\": True,\n",
    "        \"run_eval_0\": True,\n",
    "         # run_eval_1 would crash because tagger pipeline is missing in model, it's disabled\n",
    "        \"run_eval_1\": False,\n",
    "    },\n",
    "    {\n",
    "        \"model_dir\": \"ner_apis_2020-04-07_15:00:35\",\n",
    "        \"run_eval_manual\": True,\n",
    "        \"run_eval_0\": True,\n",
    "        \"run_eval_1\": True,\n",
    "    },\n",
    "    {\n",
    "        \"model_dir\": \"ner_apis_2020-04-16_14:21:46\",\n",
    "        \"run_eval_manual\": True,\n",
    "        \"run_eval_0\": True,\n",
    "        \"run_eval_1\": True,\n",
    "    },\n",
    "]\n",
    "# __sresch__ taken from evaluate_model.py, adapted only in minor parts\n",
    "for e in evaluations:\n",
    "    # ============= LOAD MODEL\n",
    "\n",
    "    print(f'Loading model at {e[\"model_dir\"]} ...')\n",
    "    nerer = ner.model_ner.NERer.from_saved(\"../\" + e['model_dir'], load_training_data=False)\n",
    "    print(\"Finished loading model.\")\n",
    "    print(f\"NER tags: {nerer.nlp.get_pipe('ner').labels}\")\n",
    "    print(f\"number of tags in evaluation data: {sum(len(e.entities) for e in nerer.evaluation_data)}\")\n",
    "    print(f\"number of sentences in evaluation data: {len(nerer.evaluation_data)}\")\n",
    "\n",
    "    # ================= MANUAL EVALUATION\n",
    "\n",
    "    if e[\"run_eval_manual\"]:\n",
    "        print(\"Starting EM (manual evaluation) ...\")\n",
    "        nerer.evaluate_manually()\n",
    "        e['EM'] = nerer.scores_manual\n",
    "        print(f\"EM: ents_p={nerer.scores_manual.p()}, ents_r={nerer.scores_manual.r()}\")\n",
    "\n",
    "    # ================= SPACY'S EVALUATION\n",
    "\n",
    "    if e[\"run_eval_0\"]:\n",
    "        print(\"Running spacy's evaluation (E0) with (string, GoldParse) as input over only the 'ner' pipe ...\")\n",
    "        assert nerer.nlp.has_pipe('ner')\n",
    "        pipes_to_disable = []\n",
    "        if nerer.nlp.has_pipe('tagger'):\n",
    "            pipes_to_disable.append('tagger')\n",
    "        if nerer.nlp.has_pipe('parser'):\n",
    "            pipes_to_disable.append('parser')\n",
    "        try:\n",
    "            nerer.evaluate(pipes_to_disable=pipes_to_disable)\n",
    "        except ValueError:\n",
    "            evaluate_without_tokenization_mismatches(nerer, pipes_to_disable)\n",
    "        scorer0 = nerer.scorer\n",
    "        e['E0'] = scorer0\n",
    "        print(f\"E0: ents_p={scorer0.ents_p}, ents_r={scorer0.ents_r}\")\n",
    "\n",
    "    if e[\"run_eval_1\"]:\n",
    "        print(\"Running spacy's evaluation (E1) with (string, GoldParse) as input over the pipes 'tagger', 'parser', and 'ner' ...\")\n",
    "        assert nerer.nlp.has_pipe('tagger')\n",
    "        assert nerer.nlp.has_pipe('parser')\n",
    "        assert nerer.nlp.has_pipe('ner')\n",
    "        pipes_to_disable = []\n",
    "        try:\n",
    "            nerer.evaluate(pipes_to_disable=pipes_to_disable)\n",
    "        except ValueError:\n",
    "            evaluate_without_tokenization_mismatches(nerer, pipes_to_disable)\n",
    "        scorer1 = nerer.scorer\n",
    "        e['E1'] = scorer1\n",
    "        print(f\"E1: ents_p={scorer1.ents_p}, ents_r={scorer1.ents_r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER tags: ('LOC', 'MISC', 'ORG', 'PER')\n",
      "number of tags in evaluation data: 3144\n",
      "number of sentences in evaluation data: 904\n",
      "p: 0.8176733780760627, r: 0.23595868302130407\n"
     ]
    }
   ],
   "source": [
    "# evaluate model 2020_04_30\n",
    "\n",
    "\n",
    "# __srech__ custom function so that the evaluation data's shape is handled. The data and its tags is\n",
    "# persisted as list of tokens, making it necessary to compare them against predicted tokens, which\n",
    "# hinders the usage of spaCy's evaluate function (to my knowledge). Meaning that precision and\n",
    "# recall are manually calculated.\n",
    "model_dir = \"ner_apis_2020-04-30_11:24:09\"\n",
    "nlp = spacy.load(f\"../{model_dir}/nlp\")\n",
    "ner_valid_list = nlp.get_pipe(\"ner\").labels\n",
    "print(f\"NER tags: {ner_valid_list}\")\n",
    "count_tp = 0\n",
    "count_fp = 0\n",
    "count_fn = 0\n",
    "count_total = 0\n",
    "count_sentences = 0\n",
    "count_tags = 0\n",
    "with open(f\"../{model_dir}/corpus/evalset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_data = json.load(f)[\"paragraphs\"]\n",
    "    for p in eval_data:\n",
    "        token_pred_list = nlp(p[\"raw\"])\n",
    "        token_real_list = []\n",
    "        for s in p[\"sentences\"]:\n",
    "            for t in s[\"tokens\"]:\n",
    "                token_real_list.append(t)\n",
    "                if t[\"ner\"] != \"O\":\n",
    "                    count_tags += 1\n",
    "            count_sentences += 1\n",
    "        for token_pred, token_real in zip(token_pred_list, token_real_list):\n",
    "            if token_pred.orth_ == token_real[\"orth\"]:\n",
    "                ner_pred = token_pred.ent_type_\n",
    "                ner_real = token_real[\"ner\"]\n",
    "                if ner_pred != \"\" or ner_real != \"O\":\n",
    "                    count_total += 1\n",
    "                    pred_is_correct = False\n",
    "                    for ner_valid in ner_valid_list:\n",
    "                        if ner_valid in ner_pred and ner_valid in ner_real:\n",
    "                            pred_is_correct = True\n",
    "                            count_tp += 1\n",
    "                            break\n",
    "                    if not pred_is_correct:\n",
    "                        if ner_pred != \"\":\n",
    "                            count_fp += 1\n",
    "                        elif ner_pred == \"\" and ner_real != \"O\" :\n",
    "                            count_fn += 1\n",
    "\n",
    "print(f\"number of tags in evaluation data: {count_tags}\")\n",
    "print(f\"number of sentences in evaluation data: {count_sentences}\")\n",
    "if count_tp + count_fp + count_fn != count_total:\n",
    "    raise Exception()\n",
    "\n",
    "p = count_tp / (count_tp + count_fp)\n",
    "r = count_tp / (count_tp + count_fn)\n",
    "print(f\"p: {p}, r: {r}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
